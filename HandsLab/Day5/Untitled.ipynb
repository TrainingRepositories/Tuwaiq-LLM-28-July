{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b86f4e7-d814-4b4c-aef4-787efc3235cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os , json , time\n",
    "\n",
    "# Load your credentials from a JSON file\n",
    "with open('.env.json', 'r') as file:\n",
    "    credentials = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e42ec6c-0227-43e2-9503-b1323d6e5138",
   "metadata": {},
   "source": [
    "# Hugging Face APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aec92dc-023a-40b6-b61b-3b99832bfcee",
   "metadata": {},
   "source": [
    "## Example 1: Sentence Completion**\n",
    "\n",
    "Let’s look at how we can use Bloom for sentence completion. The code below uses the hugging face token for API to send an API call with the input text and appropriate parameters for getting the best response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92d152bb-3b74-4ccd-8d49-603613dff236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "\n",
    "inference = InferenceClient(model = \"bigscience/bloom\",token=credentials['huggingface_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9befc65-ed62-4674-9ff8-c88c25589344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(prompt:str,\n",
    "          max_length = 128,\n",
    "          top_k = 0,\n",
    "          num_beams = 0,\n",
    "          no_repeat_ngram_size = 2,\n",
    "          top_p = 0.9,\n",
    "          seed=42,\n",
    "          temperature=0.7,\n",
    "          greedy_decoding = False,\n",
    "          return_full_text = False):\n",
    "    \n",
    "\n",
    "    top_k = None if top_k == 0 else top_k\n",
    "    do_sample = False if num_beams > 0 else not greedy_decoding\n",
    "    num_beams = None if (greedy_decoding or num_beams == 0) else num_beams\n",
    "    no_repeat_ngram_size = None if num_beams is None else no_repeat_ngram_size\n",
    "    top_p = None if num_beams else top_p\n",
    "    early_stopping = None if num_beams is None else num_beams > 0\n",
    "    \n",
    "    params = {\n",
    "            \"max_new_tokens\": max_length,\n",
    "            \"top_k\": top_k,\n",
    "            \"top_p\": top_p,\n",
    "            \"temperature\": temperature,\n",
    "            \"do_sample\": do_sample,\n",
    "            \"seed\": seed,\n",
    "            \"early_stopping\":early_stopping,\n",
    "            \"no_repeat_ngram_size\":no_repeat_ngram_size,\n",
    "            \"num_beams\":num_beams,\n",
    "            \"return_full_text\":return_full_text\n",
    "        }\n",
    "    \n",
    "    s = time.time()\n",
    "    response = inference.post(json = {\"inputs\": prompt, \"params\": params})\n",
    "    #print(f\"Processing time was {proc_time} seconds\")\n",
    "    return json.loads(response.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fcfba213-423b-4ef3-b430-6bc3069737e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The thing that makes large language models interesting is  that they are very large. The largest models are in the order of billions of parameters. This is\n"
     ]
    }
   ],
   "source": [
    "print(infer(prompt='The thing that makes large language models interesting is ')[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401fdbef-d09c-49db-8747-d1e0c877862d",
   "metadata": {},
   "source": [
    "## Example 2: Question Answers**\n",
    "\n",
    "We can use the API for the Roberta-base model which can be a source to refer to and reply to. Let’s change the payload to provide some information about myself and ask the model to answer questions based on that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb68a52-5c5c-497b-957f-da13ef466064",
   "metadata": {},
   "source": [
    "### Using PIPLINE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e71c2b6c-a479-49ce-b1e4-f05dbc04acbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d02730423f447cbc7b405faea7542b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8019f6797b421fa18476698abcf637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3507b6f524245d7a7c585eb14ef6469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/79.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b7442c35f1b4883a3b5a67257716c37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2731cb34f53046828252b1409fe2e36b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19af0fdded124ec1b391c8be04c315bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "QA_input = {\n",
    "    'question': 'Why is model conversion important?',\n",
    "    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n",
    "}\n",
    "res = nlp(QA_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b71c2a77-14dc-4a57-afed-fc9e76e547a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.21171456575393677,\n",
       " 'start': 59,\n",
       " 'end': 84,\n",
       " 'answer': 'gives freedom to the user'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae194fa6-c7a1-4dd6-b88d-1b16989735d8",
   "metadata": {},
   "source": [
    "### Without PIPLINE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a34ef120-8fe8-4ad9-8327-440d9a20791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "model_name = 'deepset/roberta-base-squad2'\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name,return_dict=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "question = \"What has Huggingface done ?\"\n",
    "text = \"Huggingface has democratized NLP. Huge thanks to Huggingface for this.\"\n",
    "\n",
    "encoding = tokenizer(question, text, return_tensors=\"pt\")\n",
    "input_ids = encoding[\"input_ids\"]\n",
    "# Transform input tokens\n",
    "\n",
    "\n",
    "\n",
    "# default is local attention everywhere\n",
    "# the forward method will automatically set global attention on question tokens\n",
    "attention_mask = encoding[\"attention_mask\"]\n",
    "\n",
    "start_scores, end_scores = model(input_ids, attention_mask=attention_mask)\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
    "\n",
    "answer_tokens = all_tokens[torch.argmax(start_scores) :torch.argmax(end_scores)+1]\n",
    "answer = tokenizer.decode(tokenizer.convert_tokens_to_ids(answer_tokens))\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1beeda9c-a4c8-48d7-84fa-cfb89a6bb7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' democratized NLP'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fc833cf-5d55-4d9f-af88-4a304fd7a254",
   "metadata": {},
   "source": [
    "## Example 3: Summarization\n",
    "\n",
    "We can summarize using Large Language Models. Let’s summarize a long text describing large language models using the Bart Large CNN model. We modify the API URL and added the input text below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b988dbd6-0dbc-49d5-ab05-411c8c7fab41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'A large language model, or LLM, is a deep learning model '\n",
      "                  'that can understand, learn, summarize, translate, predict, '\n",
      "                  'and generate text. They aren’t just for teaching AIs human '\n",
      "                  'languages, but for understanding proteins, writing software '\n",
      "                  'code, and much more.'}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/facebook/bart-large-cnn\"\n",
    "\n",
    "def query(payload):\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {credentials['huggingface_API_KEY']}\",\n",
    "    }\n",
    "    response = requests.post(API_URL, json=payload , headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "params = {'do_sample': False}\n",
    "\n",
    "full_text = '''AI applications are summarizing articles, writing stories and\n",
    "engaging in long conversations — and large language models are doing\n",
    "the heavy lifting.\n",
    "\n",
    "A large language model, or LLM, is a deep learning model that can\n",
    "understand, learn, summarize, translate, predict, and generate text and other\n",
    "content based on knowledge gained from massive datasets.\n",
    "\n",
    "Large language models - successful applications of\n",
    "transformer models. They aren’t just for teaching AIs human languages,\n",
    "but for understanding proteins, writing software code, and much, much more.\n",
    "\n",
    "In addition to accelerating natural language processing applications —\n",
    "like translation, chatbots, and AI assistants — large language models are\n",
    "used in healthcare, software development, and use cases in many other fields.'''\n",
    "\n",
    "output = query({\n",
    "    'inputs': full_text,\n",
    "    'parameters': params\n",
    "})\n",
    "\n",
    "pprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f0acb4b5-6ddd-4f05-8fc5-2b3fbe38ce15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'A large language model, or LLM, is a deep learning model that can understand, learn, summarize, translate, predict, and generate text. They aren’t just for teaching AIs human languages, but for understanding proteins, writing software code, and much more.'}]\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2bbd36-4895-4308-b9f6-214c7bfec889",
   "metadata": {},
   "source": [
    "## Example 4 langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "34943b96-c718-4850-9a5d-2425a0299e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6de86aea-4b24-4017-b5c9-9316f13018b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import load_tool, ReactCodeAgent, HfEngine\n",
    "from langchain.agents import load_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fc210ac0-cb7c-4ff7-bf19-3e4e29b587b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're loading a tool from the Hub from None. Please make sure this is a source that you trust as the code within that tool will be executed on your machine. Always verify the code of the tools that you load. We recommend specifying a `revision` to ensure you're loading the code that you have checked.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b416630b2b42c8a75b1faffbb7da02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tool_config.json:   0%|          | 0.00/412 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78944934045944ff8df2c3f7f66d7325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tool.py:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/spaces/m-ric/text-to-image:\n",
      "- tool.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "\u001b[33;1m======== New task ========\u001b[0m\n",
      "\u001b[37;1mGenerate an image of a violet boat on a lake, with a red comet in the sky.\u001b[0m\n",
      "\u001b[31;20mError in generating llm output: (ReadTimeoutError(\"HTTPSConnectionPool(host='api-inference.huggingface.co', port=443): Read timed out. (read timeout=120)\"), '(Request ID: a78e5584-0612-4845-80cf-0777c3d52898)').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
      "    response = conn.getresponse()\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/urllib3/connection.py\", line 464, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py\", line 318, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py\", line 279, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py\", line 1307, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py\", line 1163, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/requests/adapters.py\", line 667, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n",
      "    retries = retries.increment(\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/urllib3/util/retry.py\", line 474, in increment\n",
      "    raise reraise(type(error), error, _stacktrace)\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/urllib3/util/util.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n",
      "    response = self._make_request(\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 538, in _make_request\n",
      "    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 369, in _raise_timeout\n",
      "    raise ReadTimeoutError(\n",
      "urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='api-inference.huggingface.co', port=443): Read timed out. (read timeout=120)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/transformers/agents/agents.py\", line 1026, in step\n",
      "    llm_output = self.llm_engine(self.prompt, stop_sequences=[\"<end_action>\", \"Observation:\"])\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/transformers/agents/llm_engine.py\", line 78, in __call__\n",
      "    response = self.client.chat_completion(messages, stop=stop_sequences, max_tokens=1500)\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/huggingface_hub/inference/_client.py\", line 837, in chat_completion\n",
      "    data = self.post(\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/huggingface_hub/inference/_client.py\", line 289, in post\n",
      "    response = get_session().post(\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/requests/sessions.py\", line 637, in post\n",
      "    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 66, in send\n",
      "    return super().send(request, *args, **kwargs)\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/requests/adapters.py\", line 713, in send\n",
      "    raise ReadTimeout(e, request=request)\n",
      "requests.exceptions.ReadTimeout: (ReadTimeoutError(\"HTTPSConnectionPool(host='api-inference.huggingface.co', port=443): Read timed out. (read timeout=120)\"), '(Request ID: a78e5584-0612-4845-80cf-0777c3d52898)')\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/transformers/agents/agents.py\", line 756, in direct_run\n",
      "    step_logs = self.step()\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/transformers/agents/agents.py\", line 1028, in step\n",
      "    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\n",
      "transformers.agents.agents.AgentGenerationError: Error in generating llm output: (ReadTimeoutError(\"HTTPSConnectionPool(host='api-inference.huggingface.co', port=443): Read timed out. (read timeout=120)\"), '(Request ID: a78e5584-0612-4845-80cf-0777c3d52898)').\n",
      "\u001b[31;20mError in code parsing: \n",
      "The code blob you used is invalid: due to the following error: 'NoneType' object has no attribute 'group'\n",
      "This means that the regex pattern ```(?:py|python)?\\n(.*?)\\n``` was not respected: make sure to include code with the correct pattern, for instance:\n",
      "Thoughts: Your thoughts\n",
      "Code:\n",
      "```py\n",
      "# Your python code here\n",
      "```<end_action>. Make sure to provide correct code\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/transformers/agents/agents.py\", line 116, in parse_code_blob\n",
      "    return match.group(1).strip()\n",
      "AttributeError: 'NoneType' object has no attribute 'group'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/transformers/agents/agents.py\", line 1043, in step\n",
      "    code_action = parse_code_blob(raw_code_action)\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/transformers/agents/agents.py\", line 118, in parse_code_blob\n",
      "    raise ValueError(\n",
      "ValueError: \n",
      "The code blob you used is invalid: due to the following error: 'NoneType' object has no attribute 'group'\n",
      "This means that the regex pattern ```(?:py|python)?\\n(.*?)\\n``` was not respected: make sure to include code with the correct pattern, for instance:\n",
      "Thoughts: Your thoughts\n",
      "Code:\n",
      "```py\n",
      "# Your python code here\n",
      "```<end_action>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/transformers/agents/agents.py\", line 756, in direct_run\n",
      "    step_logs = self.step()\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/transformers/agents/agents.py\", line 1046, in step\n",
      "    raise AgentParsingError(error_msg)\n",
      "transformers.agents.agents.AgentParsingError: Error in code parsing: \n",
      "The code blob you used is invalid: due to the following error: 'NoneType' object has no attribute 'group'\n",
      "This means that the regex pattern ```(?:py|python)?\\n(.*?)\\n``` was not respected: make sure to include code with the correct pattern, for instance:\n",
      "Thoughts: Your thoughts\n",
      "Code:\n",
      "```py\n",
      "# Your python code here\n",
      "```<end_action>. Make sure to provide correct code\n",
      "\u001b[31;20mError in generating llm output: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1/v1/chat/completions (Request ID: DnDD0uLy1G7xKwCzzuONU)\n",
      "\n",
      "Rate limit reached. Please log in or use a HF access token.\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/requests/models.py\", line 1024, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1/v1/chat/completions\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/transformers/agents/agents.py\", line 1026, in step\n",
      "    llm_output = self.llm_engine(self.prompt, stop_sequences=[\"<end_action>\", \"Observation:\"])\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/transformers/agents/llm_engine.py\", line 78, in __call__\n",
      "    response = self.client.chat_completion(messages, stop=stop_sequences, max_tokens=1500)\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/huggingface_hub/inference/_client.py\", line 837, in chat_completion\n",
      "    data = self.post(\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/huggingface_hub/inference/_client.py\", line 304, in post\n",
      "    hf_raise_for_status(response)\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 371, in hf_raise_for_status\n",
      "    raise HfHubHTTPError(str(e), response=response) from e\n",
      "huggingface_hub.utils._errors.HfHubHTTPError: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1/v1/chat/completions (Request ID: DnDD0uLy1G7xKwCzzuONU)\n",
      "\n",
      "Rate limit reached. Please log in or use a HF access token\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/transformers/agents/agents.py\", line 756, in direct_run\n",
      "    step_logs = self.step()\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/transformers/agents/agents.py\", line 1028, in step\n",
      "    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\n",
      "transformers.agents.agents.AgentGenerationError: Error in generating llm output: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1/v1/chat/completions (Request ID: DnDD0uLy1G7xKwCzzuONU)\n",
      "\n",
      "Rate limit reached. Please log in or use a HF access token.\n",
      "\u001b[31;20mError in generating llm output: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1/v1/chat/completions (Request ID: o7xVeZUU33hoB2o0zVmPq)\n",
      "\n",
      "Rate limit reached. Please log in or use a HF access token.\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/requests/models.py\", line 1024, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1/v1/chat/completions\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/transformers/agents/agents.py\", line 1026, in step\n",
      "    llm_output = self.llm_engine(self.prompt, stop_sequences=[\"<end_action>\", \"Observation:\"])\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/transformers/agents/llm_engine.py\", line 78, in __call__\n",
      "    response = self.client.chat_completion(messages, stop=stop_sequences, max_tokens=1500)\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/huggingface_hub/inference/_client.py\", line 837, in chat_completion\n",
      "    data = self.post(\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/huggingface_hub/inference/_client.py\", line 304, in post\n",
      "    hf_raise_for_status(response)\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 371, in hf_raise_for_status\n",
      "    raise HfHubHTTPError(str(e), response=response) from e\n",
      "huggingface_hub.utils._errors.HfHubHTTPError: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1/v1/chat/completions (Request ID: o7xVeZUU33hoB2o0zVmPq)\n",
      "\n",
      "Rate limit reached. Please log in or use a HF access token\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/transformers/agents/agents.py\", line 756, in direct_run\n",
      "    step_logs = self.step()\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/transformers/agents/agents.py\", line 1028, in step\n",
      "    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\n",
      "transformers.agents.agents.AgentGenerationError: Error in generating llm output: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1/v1/chat/completions (Request ID: o7xVeZUU33hoB2o0zVmPq)\n",
      "\n",
      "Rate limit reached. Please log in or use a HF access token.\n",
      "\u001b[31;20mError in generating llm output: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1/v1/chat/completions (Request ID: H9jiIowdk4hG4cXWp08NF)\n",
      "\n",
      "Rate limit reached. Please log in or use a HF access token.\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/requests/models.py\", line 1024, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1/v1/chat/completions\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/transformers/agents/agents.py\", line 1026, in step\n",
      "    llm_output = self.llm_engine(self.prompt, stop_sequences=[\"<end_action>\", \"Observation:\"])\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/transformers/agents/llm_engine.py\", line 78, in __call__\n",
      "    response = self.client.chat_completion(messages, stop=stop_sequences, max_tokens=1500)\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/huggingface_hub/inference/_client.py\", line 837, in chat_completion\n",
      "    data = self.post(\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/huggingface_hub/inference/_client.py\", line 304, in post\n",
      "    hf_raise_for_status(response)\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 371, in hf_raise_for_status\n",
      "    raise HfHubHTTPError(str(e), response=response) from e\n",
      "huggingface_hub.utils._errors.HfHubHTTPError: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1/v1/chat/completions (Request ID: H9jiIowdk4hG4cXWp08NF)\n",
      "\n",
      "Rate limit reached. Please log in or use a HF access token\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/transformers/agents/agents.py\", line 756, in direct_run\n",
      "    step_logs = self.step()\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/transformers/agents/agents.py\", line 1028, in step\n",
      "    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\n",
      "transformers.agents.agents.AgentGenerationError: Error in generating llm output: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1/v1/chat/completions (Request ID: H9jiIowdk4hG4cXWp08NF)\n",
      "\n",
      "Rate limit reached. Please log in or use a HF access token.\n",
      "\u001b[31;20mError in generating llm output: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1/v1/chat/completions (Request ID: z29bM7h6t8aY7_kXYoqPD)\n",
      "\n",
      "Rate limit reached. Please log in or use a HF access token.\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/requests/models.py\", line 1024, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1/v1/chat/completions\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/transformers/agents/agents.py\", line 1026, in step\n",
      "    llm_output = self.llm_engine(self.prompt, stop_sequences=[\"<end_action>\", \"Observation:\"])\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/transformers/agents/llm_engine.py\", line 78, in __call__\n",
      "    response = self.client.chat_completion(messages, stop=stop_sequences, max_tokens=1500)\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/huggingface_hub/inference/_client.py\", line 837, in chat_completion\n",
      "    data = self.post(\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/huggingface_hub/inference/_client.py\", line 304, in post\n",
      "    hf_raise_for_status(response)\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 371, in hf_raise_for_status\n",
      "    raise HfHubHTTPError(str(e), response=response) from e\n",
      "huggingface_hub.utils._errors.HfHubHTTPError: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1/v1/chat/completions (Request ID: z29bM7h6t8aY7_kXYoqPD)\n",
      "\n",
      "Rate limit reached. Please log in or use a HF access token\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/transformers/agents/agents.py\", line 756, in direct_run\n",
      "    step_logs = self.step()\n",
      "  File \"/Users/abdulwahabmac/Desktop/MyFiles/Projects/Training/Tuwaiq/Tuwaiq-LLM-28-July/.env/lib/python3.10/site-packages/transformers/agents/agents.py\", line 1028, in step\n",
      "    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\n",
      "transformers.agents.agents.AgentGenerationError: Error in generating llm output: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1/v1/chat/completions (Request ID: z29bM7h6t8aY7_kXYoqPD)\n",
      "\n",
      "Rate limit reached. Please log in or use a HF access token.\n",
      "\u001b[31;20mReached max iterations.\u001b[0m\n",
      "NoneType: None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Error in generating final llm output: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1/v1/chat/completions (Request ID: XkNZ5dLBxAzyspGjiIZPf)\\n\\nRate limit reached. Please log in or use a HF access token.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import load_tool, ReactCodeAgent, HfEngine\n",
    "from langchain.agents import load_tools\n",
    "\n",
    "image_tools = load_tool(\"m-ric/text-to-image\")\n",
    "\n",
    "mistral_engine = HfEngine(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "agent = ReactCodeAgent(tools=[image_tools], llm_engine= mistral_engine)\n",
    "\n",
    "purple_alien = agent.run(\n",
    "    \"Generate an image of a violet boat on a lake, with a red comet in the sky.\",\n",
    ")\n",
    "purple_alien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f2ea4acd-3e98-4314-b095-6e74b99e46bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Error in generating final llm output: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1/v1/chat/completions (Request ID: XkNZ5dLBxAzyspGjiIZPf)\\n\\nRate limit reached. Please log in or use a HF access token.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purple_alien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1996e3-3ae3-4c16-b277-77946a26a8d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
