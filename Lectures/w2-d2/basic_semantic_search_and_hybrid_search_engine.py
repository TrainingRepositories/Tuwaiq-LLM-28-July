# -*- coding: utf-8 -*-
"""Basic_semantic_search_and_hybrid_search_engine.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OZ8vj_BHhsPJPWlbXdZpvyZU5gEuNr1_
"""

! pip install sentence-transformers scikit-learn

from sentence_transformers import SentenceTransformer, util
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import nltk
nltk.download('stopwords')

from sentence_transformers import SentenceTransformer, util
import numpy as np

# Load the pre-trained model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Example documents
documents = [
    "Machine learning is fascinating.",
    "Artificial Intelligence and machine learning are closely related.",
    "I love programming in Python.",
    "Natural Language Processing is a subset of AI.",
    "Python is a versatile programming language."
]

# Encode the documents
document_embeddings = model.encode(documents, convert_to_tensor=True)

def semantic_search(query, top_k=3):
    # Encode the query
    query_embedding = model.encode(query, convert_to_tensor=True)

    # Compute cosine similarities between the query and the documents
    cosine_scores = util.pytorch_cos_sim(query_embedding, document_embeddings)[0]

    # Find the top_k highest scores
    top_results = np.argpartition(-cosine_scores, range(top_k))[:top_k]

    print(f"Query: {query}\nTop {top_k} most similar documents:")
    for idx in top_results:
        print(f"Document: {documents[idx]}, Score: {cosine_scores[idx]:.4f}")

# Example usage
while True:
    user_query = input("Enter your query (or type 'exit' to quit): ")
    if user_query.lower() == 'exit':
        break
    semantic_search(user_query)

"""To convert the basic semantic search engine into an advanced one, we can implement several improvements and features such as:

* Indexing for Faster Searches: Use an efficient indexing structure like FAISS for fast nearest neighbor search.
* Preprocessing Text: Implement text preprocessing (e.g., tokenization, stemming, removing stop words) to improve the quality of embeddings.
* User Interface: Add a user-friendly interface, possibly using a web framework like Flask or FastAPI.
* Handling Large Datasets: Implement methods to handle and search through large datasets.
* Improved Ranking: Combine semantic similarity with other ranking factors like document popularity or recency.

#  integrate TF-IDF with the existing semantic search engine
# The hybrid method considers both semantic and syntactic similarities.
"""

from sentence_transformers import SentenceTransformer, util
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Load the pre-trained model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Example documents
documents = [
    "Machine learning is fascinating.",
    "Artificial Intelligence and machine learning are closely related.",
    "I love programming in Python.",
    "Natural Language Processing is a subset of AI.",
    "Python is a versatile programming language."
]

# Encode the documents using SentenceTransformers
document_embeddings = model.encode(documents, convert_to_tensor=True)

# Create a TF-IDF Vectorizer and fit on the documents
tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(documents)

def hybrid_search(query, top_k=3):
    # Encode the query using SentenceTransformers
    query_embedding = model.encode([query], convert_to_tensor=True)

    # Compute cosine similarities between the query and the documents using SentenceTransformers
    cosine_scores = util.pytorch_cos_sim(query_embedding, document_embeddings)[0]

    # Transform the query using the TF-IDF vectorizer
    query_tfidf = tfidf_vectorizer.transform([query])

    # Calculate cosine similarity between the query and the documents using TF-IDF
    tfidf_similarities = cosine_similarity(query_tfidf, tfidf_matrix)[0]

    # Combine the scores from both methods
    combined_scores = (cosine_scores.cpu().numpy() + tfidf_similarities) / 2

    # Find the top_k highest scores
    top_results = np.argpartition(-combined_scores, range(top_k))[:top_k]

    print(f"Query: {query}\nTop {top_k} most similar documents:")
    for idx in top_results:
        print(f"Document: {documents[idx]}, Combined Score: {combined_scores[idx]:.4f}")

# Example usage
while True:
    user_query = input("Enter your query (or type 'exit' to quit): ")
    if user_query.lower() == 'exit':
        break
    hybrid_search(user_query)

"""## Hybrid Search Approach
* The hybrid approach of combining Sentence Transformers and TF-IDF is generally sound. It leverages the strengths of both methods:

* Sentence Transformers: Captures semantic similarity based on the meaning of the text.
* TF-IDF: Focuses on term frequency and inverse document frequency, which is useful for identifying keyword-based matches.
By combining these two methods, the search system can potentially achieve better overall performance than using either method alone

# Let's make it a little more complicated.:

* Weighting of scores: The combination of cosine scores from both methods is done by simple averaging. Consider using weighted averages based on performance evaluation to optimize results.
* Normalization: Normalizing the combined scores might improve comparability across different query-document pairs.

* Query preprocessing: Implementing basic query preprocessing techniques like stemming, lemmatization, and stop word removal can enhance search accuracy. **bold text**
Evaluation metrics: Incorporate metrics like Mean Average Precision (MAP), Normalized Discounted Cumulative Gain (NDCG), or Recall@k to evaluate the search performance and refine the model.
"""

from sklearn.metrics import average_precision_score

# Load the pre-trained model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Example documents
documents = [
    "Machine learning is fascinating.",
    "Artificial Intelligence and machine learning are closely related.",
    "I love programming in Python.",
    "Natural Language Processing is a subset of AI.",
    "Python is a versatile programming language."
]

# Encode the documents using SentenceTransformers
document_embeddings = model.encode(documents, convert_to_tensor=True)

# Create a TF-IDF Vectorizer and fit on the documents
tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(documents)

def preprocess_query(query):
    # Tokenization, stop word removal, and stemming
    stop_words = set(stopwords.words('english'))
    stemmer = PorterStemmer()
    tokens = query.lower().split()
    filtered_tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]
    return ' '.join(filtered_tokens)

def hybrid_search(query, top_k=3, weight_st=0.5, weight_tfidf=0.5):
    # Encode the query using SentenceTransformers
    query_preprocessed = preprocess_query(query)
    query_embedding = model.encode([query_preprocessed], convert_to_tensor=True)

    # Compute cosine similarities between the query and the documents using SentenceTransformers
    cosine_scores = util.pytorch_cos_sim(query_embedding, document_embeddings)[0]

    # Transform the query using the TF-IDF vectorizer
    query_tfidf = tfidf_vectorizer.transform([query_preprocessed])

    # Calculate cosine similarity between the query and the documents using TF-IDF
    tfidf_similarities = cosine_similarity(query_tfidf, tfidf_matrix)[0]
    #print(tfidf_similarities)

    # Combine the scores with weights
    combined_scores = (weight_st * cosine_scores.cpu().numpy() + weight_tfidf * tfidf_similarities) / (weight_st + weight_tfidf)
    #print(combined_scores)

    # Normalize scores (optional)
    combined_scores = (combined_scores - np.min(combined_scores)) / (np.max(combined_scores) - np.min(combined_scores))

    # Find the top_k highest scores
    top_results = np.argpartition(-combined_scores, range(top_k))[:top_k]
    #print(top_results)

    print(f"Query: {query}\nTop {top_k} most similar documents:")
    for idx in top_results:
        print(f"Document: {documents[idx]}, Combined Score: {combined_scores[idx]:.4f}")

    return top_results, combined_scores


# Example usage with evaluation:
while True:
    user_query = input("Enter your query (or type 'exit' to quit): ")
    if user_query.lower() == 'exit':
        break
    hybrid_search(user_query)

    predicted_ranks, _ = hybrid_search(user_query)

